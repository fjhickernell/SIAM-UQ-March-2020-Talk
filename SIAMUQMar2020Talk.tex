%Talk given virtually for SIAM UQ 2020 March
\documentclass[10pt,compress,xcolor={usenames,dvipsnames},aspectratio=169]{beamer}
%\documentclass[xcolor={usenames,dvipsnames},aspectratio=169]{beamer} %slides and 
%notes
\usepackage{amsmath,
	amssymb,
	datetime,
	mathtools,
	bbm,
	%mathabx,
	array,
	booktabs,
	xspace,
	calc,
	colortbl,
 	graphicx}
\usepackage[usenames]{xcolor}
\usepackage[giveninits=false,backend=biber,style=nature, maxcitenames =10, mincitenames=9]{biblatex}
\addbibresource{FJHown23.bib}
\addbibresource{FJH23.bib}
\usepackage{newpxtext}
\usepackage[euler-digits,euler-hat-accent]{eulervm}
\usepackage{media9}
\usepackage[autolinebreaks]{mcode}
\usepackage[tikz]{mdframed}


\usetheme{FJHSlimNoFoot169}
\setlength{\parskip}{2ex}
\setlength{\arraycolsep}{0.5ex}

\DeclareMathOperator{\sol}{SOL}
\DeclareMathOperator{\app}{APP}
\DeclareMathOperator{\alg}{ALG}
\DeclareMathOperator{\ACQ}{ACQ}
\DeclareMathOperator{\ERR}{ERR}
\DeclareMathOperator{\COST}{COST}
\DeclareMathOperator{\COMP}{COMP}
\newcommand{\dataN}{\bigl(\hf(\vk_i)\bigr)_{i=1}^n}
\newcommand{\dataNj}{\bigl(\hf(\vk_i)\bigr)_{i=1}^{n_j}}
\newcommand{\dataNjd}{\bigl(\hf(\vk_i)\bigr)_{i=1}^{n_{j^\dagger}}}
\newcommand{\ERRN}{\ERR\bigl(\dataN,n\bigr)}

\newcommand{\Sapp}{S_{\textup{app}}}
\newcommand{\LambdaStd}{\Lambda^{\textup{std}}}
\newcommand{\LambdaSer}{\Lambda^{\textup{ser}}}
\newcommand{\LambdaAll}{\Lambda^{\textup{all}}}
\newcommand{\oton}{1\!:\!n}
\newcommand{\talert}[1]{\alert{\text{#1}}}
\DeclareMathOperator{\init}{init}
\DeclareMathOperator{\GP}{\cg\cp}
\newcommand{\MLE}{\textup{EB}}
\newcommand{\mCtheta}{{\mathsf{C}_{\vtheta}}}
\newcommand{\mCInv}{\mathsf{C}^{-1}}

%\DeclareMathOperator{\app}{app}

\providecommand{\HickernellFJ}{H.}


\iffalse
The Successes and Challenges of Automatic Bayesian Cubature

The promise of Bayesian cubature is that with reasonable prior information (or assumptions) about the integrand, one can construct an optimal approximation to the corresponding (multidimensional) integral and simultaneously a credible interval.  Automatic Bayesian cubature uses increases the sample size until half-width of the credible interval is small enough.  We discuss how to choose appropriate covariance kernels, estimate their hyper-parameters, and construct the credible intervals, all with reasonable computational effort.  We also evaluate the performance of automatic Bayesian cubature on a variety of examples.


\fi

\renewcommand{\OffTitleLength}{-10ex}
\setlength{\FJHThankYouMessageOffset}{-8ex}
\title{The Successes and Challenges of Automatic Bayesian Cubature}
\author[]{Fred J. Hickernell}
\institute{Department of Applied Mathematics \\
	Center for Interdisciplinary Scientific Computation \\  Illinois Institute of Technology \\
	\href{mailto:hickernell@iit.edu}{\url{hickernell@iit.edu}} \quad
	\href{http://mypages.iit.edu/~hickernell}{\url{mypages.iit.edu/~hickernell}}}

\thanksnote{with Jagadeeswaran R. and the GAIL team \\
	partially supported by  NSF-DMS-1522687 and NSF-DMS-1638521 (SAMSI) \\[2ex]
	Thanks to the special session organizers
	
}
\event{SIAM-UQ 2020}
\date[]{March 2020}

\input FJHDef.tex


\newlength{\figwidth}
\setlength{\figwidth}{0.25\textwidth}

\newlength{\figwidthSmall}
\setlength{\figwidthSmall}{0.2\textwidth}

\newcommand{\financePict}{\href{http://i2.cdn.turner.com/money/dam/assets/130611131918-chicago-board-options-exchange-1024x576.jpg}{\includegraphics[width
		= 3cm]{ProgramsImages/130611131918-chicago-board-options-exchange-1024x576.jpg}}}
	
	\newcommand{\scoop}[1]{\parbox{#1}{\includegraphics[width=#1]{IceCreamScoop.eps}}\xspace}
	\newcommand{\smallscoop}{\scoop{1cm}}
	\newcommand{\medscoop}{\scoop{1.8cm}}
	\newcommand{\largescoop}{\scoop{3cm}}
	\newcommand{\ICcone}[1]{\parbox{#1}{\includegraphics[width=#1,angle=270]{MediumWaffleCone.eps}}\xspace}
	\newcommand{\medcone}{\ICcone{1.2cm}}
	\newcommand{\largercone}{\parbox{2.2cm}{\vspace*{-0.2cm}\includegraphics[width=1cm,angle=270]{MediumWaffleCone.eps}}\xspace}
	\newcommand{\largecone}{\ICcone{1.8cm}}
	\newcommand{\smallcone}{\parbox{1.1cm}{\includegraphics[width=0.5cm,angle=270]{MediumWaffleCone.eps}}\xspace}

	

\newcommand{\northeaststuff}[3]{
	\begin{tikzpicture}[remember picture, overlay]
	\node [shift={(-#1 cm,-#2 cm)}]  at (current page.north east){#3};
	\end{tikzpicture}}


\begin{document}
	\tikzstyle{every picture}+=[remember picture]
	\everymath{\displaystyle}

\frame{\titlepage}


\section{Introduction}

\begin{frame}
	{The Guaranteed Automatic Integration Library (GAIL) and QMCPy Teams}
	
	\vspace{-2ex}
	\includegraphics[angle = 180, origin = c, width = 0.32\textwidth]{ProgramsImages/GAIL2014RE.jpeg} \
	\includegraphics[width = 0.32\textwidth]{ProgramsImages/GAILatSIAM2018Hi.jpeg} \ 
	\includegraphics[width = 0.32\textwidth]{ProgramsImages/GAILatChinatown2018.jpg}
	
	\vspace{-3ex}
	\begin{tabular}{p{0.55\textwidth}p{0.42\textwidth}}
		
			
		\begin{itemize}
			\item Sou-Cheng Choi (Chief Data Scientist, Kamakura)
			
			\item Yuhan Ding (IIT PhD '15, Lecturer, IIT)
			
			\item Lan Jiang  (IIT PhD '16, Compass)
			
			\item Llu\'is Antoni Jim\'enez Rugama (IIT PhD '17, UBS)
			
			\item Jagadeeswaran Rathinavel (IIT PhD '19, Wi-Tronix)
			
			\item Aleksei Sorokin (IIT BS + MAS '21 exp.)
	
			
		\end{itemize}
	
	&
	
		\begin{itemize}
	
	
	\item Tong Xin (IIT MS, UIC PhD '20 exp.)
	
	\item Kan Zhang (IIT PhD '20 exp.)
	
	\item Yizhi Zhang (IIT PhD '18, Jamran Int'l)
	
	\item Xuan Zhou (IIT PhD '15, JP Morgan)
	
	\item and others
	
	
\end{itemize}

Adaptive software libraries \href{https://gailgithub.github.io/GAIL_Dev/}{\beamerbutton{GAIL}} and \href{https://qmcsoftware.github.io/QMCSoftware/}{\beamerbutton{QMCPy}}

	
	\end{tabular}
\end{frame}


\begin{frame}{Problem}
	\only<1-4>{\northeaststuff{1}{2.4}{\includegraphics[width=2cm]{ProgramsImages/GoldVase.jpg}}}
	\only<5->{\northeaststuff{1}{2.4}{\includegraphics[width=2cm]{ProgramsImages/decorshore-fired-gold-vase-with-striped-crackled-glass-mosaic-vases.jpg}}}
	\northeaststuff{1}{0.9}{\alert{$f$}}

\vspace{-8ex}

   Want \alert{fixed tolerance cubature} $\alg: L^1[0,1]^d \times (0,\infty) \to \reals$ such that 
   
   \vspace{-4ex}
    \[
    \only<2->{\Prob_f \Bigg [ }\biggabs{\int_{[0,1]^d} f(\vx) \, \dif \vx - \alg(f,\varepsilon)} \le \varepsilon \only<2->{\Bigg ] \ge 99\%} \qquad \forall \varepsilon > 0, \quad \only<1>{\talert{for reasonable $f$}}\only<2->{\alert{f \sim \GP(m,s^2C_{\vtheta})}}
    \]
    \vspace{-3ex}
    \uncover<3->{\begin{gather*}
        \text{\alert{design or node array} }\mX = (\vx_1, \ldots, \vx_n)^T \in [0,1]^{n \times d}, \qquad \text{\alert{function data} }  \vf = f(\mX)\in \reals^n \\
    c_{0\vtheta} = \int_{[0,1]^{d} \times [0,1]^{d}} C_{\vtheta}(\vt,\vx) \, \dif \vt \dif \vx > 0, \quad 
\vc_\vtheta = \int_{[0,1]^{d}} C_{\vtheta}(\mX,\vt) \, \dif \vt \in [0,1]^{n} , \quad
\mC_\vtheta = C_{\vtheta}(\mX,\mX)  \in [0,1]^{n \times n} \\
\int_{[0,1]^d} f(\vx) \, \dif \vx  \, \big \vert \, (\vf = \vy) \sim \cn\Bigl(\underbrace{ m[ 1 - \vc_\vtheta^T \mC_\vtheta^{-1}\vone] + \vc_\vtheta^T \mC^{-1}\vy}_{\alg(f,\varepsilon)}, s^2(c_\vtheta - \vc_\vtheta^T \mC_\vtheta^{-1}\vc_\vtheta)\Bigr)
    \end{gather*}
    
    \vspace{-4ex}
Choosing $n$  large enough to make $2.58s\sqrt{c - \vc^T \mC^{-1}\vc} \le \varepsilon$ would seem to achieve our goal

 
 \vspace{-1ex}
 \uncover<4->{
 	\alert{Issues requiring attention}
 	
 	\vspace{-3ex}
 \begin{itemize}
 	\item Inference of $m$, $s$,  $\vtheta$, and $C_{\vtheta}$, which urn  $f$ comes from
 	\item Ill-conditioning and numerical cost of vector-matrix calculations
 	\item Whether Gaussian process is a reasonable assumption
\end{itemize}}}
    
\end{frame}

\begin{frame}{Inferring Gaussian Process Parameters for $\GP(m,s^2C_{\vtheta})$\footfullcite{RatHic19a}}
\only<1->{\northeaststuff{1}{2.4}{\includegraphics[width=2cm]{ProgramsImages/GoldVase.jpg}}}
\only<2->{\northeaststuff{1}{2.4}{\includegraphics[width=2cm]{ProgramsImages/decorshore-fired-gold-vase-with-striped-crackled-glass-mosaic-vases.jpg}}}
\northeaststuff{1}{0.9}{\alert{$f$}}

\vspace{-10ex}

	Using \alert{empirical Bayes}
	\begin{gather*} 
m_{\MLE} = \frac{\vone^T \mCInv_\vtheta \vy }{ \vone^T \mCInv_\vtheta \vone}, \qquad
s^2_{\MLE} = 
\frac{1}{n}
\vy^T 
\left[ \mCInv_\vtheta - 
\frac{ \mCInv_\vtheta \vone \vone^T \mCInv_\vtheta }{\vone^T\mCInv_\vtheta \vone}
\right] \vy, \\
\vtheta_{\MLE}
= \argmin_{\vtheta} \biggl \{
\log\left(\vy^T 
\left[ \mC_\vtheta^{-1} - 
\frac{ \mCInv_\vtheta \vone \vone^T \mCInv_\vtheta }{\vone^T\mCInv_\vtheta \vone}
\right] \vy 
\right)  
+  \frac{1}{n} \log(\det(\mC_\vtheta))
\biggr \}, \\
\alg(f,\varepsilon) = 
\left(
\frac{ (1 - \vone^T  \mCInv_\vtheta\vc_{\vtheta} )  \vone }{ \vone^T \mCInv_\vtheta \vone}   +  \vc_{\vtheta} 
\right)^T  \mCInv_\vtheta \vy \qquad \text{when }
2.58 s_{\mathsf{\MLE}} \sqrt{c_{0\vtheta} - \vc_{\vtheta}^T\mC_\vtheta^{-1}\vc_{\vtheta} } \le \varepsilon
	\end{gather*}
	
	\vspace{-4ex}
	
\uncover<2->{\begin{itemize}
		\item Ill-conditioning and numerical cost of vector-matrix calculations
		\item Whether Gaussian process is a reasonable assumption
\end{itemize}}

\end{frame}

\section{Fast Bayesian Transforms}

\begin{frame}{Fast Bayesian Transforms in General}
	
	\vspace{-3ex} 
	
Find a kernel $C_\vtheta$ to \alert{match} the design $\mX$ so that

\vspace{-6ex}
\begin{gather*}
\mC_\vtheta = \frac 1n \mV \mLambda \mV^H , 
\quad \quad \mV^H = n \mV^{-1}, \qquad
\mV = (\vv_1,\ldots,\vv_n)^T = (\vV_1,\ldots,\vV_n) \text{ known analytically} \\
\vv_1 = \vV_1 = \vone, \qquad \vc_\vtheta = \vone, \qquad
	\widetilde{\vb} := \mV^H \vb  \text{ requires only $\Order(n \log(n))$ operations } \forall \vb.
\end{gather*}
$C_\vtheta$ is a \alert{fast Bayesian transform kernel} and $\vb \mapsto \mV^H \vb$ a \alert{fast Bayesian transform (FBT)} 

\vspace{-2ex}
Then by empirical Bayes
\begin{gather*}
\widetilde{\vy} = \talert{\,FBT of function data } \vy , \qquad \vlambda_\vtheta = (\lambda_1 , \ldots, \lambda_n)^T=  \widetilde{\vC}_{\vtheta,1} = \talert{\,FBT of first column of } \mC_\vtheta \\
	\vtheta_{\MLE} = 
\argmin_{\vtheta}
\left[
\log\left(
\sum_{i=\alert{2}}^n \frac{\abs{\widetilde{y}_i}^2}{\lambda_{\vtheta, i}}
\right) 
+ \frac{1}{n}\sum_{i=1}^n \log(\lambda_{\vtheta,i})
\right]\\
\alg(f,\varepsilon) =  \frac{\widetilde{y}_1}{n} = \frac 1n \sum_{i=1}^n y_i = \talert{ sample mean}
\qquad \text{when }
\frac{2.58}{n}\sqrt{
	\sum_{i=2}^{n} \frac{\abs{\widetilde{y}_i}^2}{\lambda_{\vtheta,i}}  
	\,
	\left( 1 -  \frac{n}{\lambda_{\vtheta,i}} \right) 
} \le \varepsilon
\end{gather*}
Cost is \alert{$O(n \log (n))$} times the number of iterations for optimizing $\vtheta$

\end{frame}

\begin{frame}{Types of FBT Kernels}
	
	\vspace{-6ex}
	\begin{tabular}{>{\centering}p{0.47\textwidth}@{\qquad}>{\centering}p{0.47\textwidth}}
		\multicolumn{2}{>{\centering}p{\textwidth}}{$C_\vtheta(\vt,\vx) = K_\vtheta(\vx \ominus \vt)$, \quad
			$\{\vx_i\}_{i=1}^{2^m} = $ affine shift of a group under $\oplus$ for $m = 0, 1, \ldots$} \tabularnewline
		\uncover<2->{Shifted Lattice Nodes, $\oplus = $ addition $\bmod \vone$} & 
	\uncover<2->{Scrambled Sobol' Nodes, $\oplus = $ bitwise addition}  \tabularnewline
		\uncover<2->{\includegraphics[height = 4cm]{ProgramsImages/ShiftedLatticePoints.eps}} &
		\uncover<2->{\includegraphics[height = 4cm]{ProgramsImages/SSobolPoints.eps}} \tabularnewline
		\uncover<3->{FBT  $ = $ \alert{Fast Fourier Transform}} &
			\uncover<3->{FBT $ = $ \alert{Fast Walsh Transform}}
	\end{tabular}
\end{frame}

\begin{frame}{Flexible FBT Kernel for Lattice Node Designs}
	
\vspace{-4ex}

$C_\vtheta(\vt,\vx) = K_\vtheta(\vx \ominus \vt)$ must be \alert{positive definite}, where
$\oplus = $ addition $\bmod \vone$.  Common example is 

\vspace{-6ex}
\begin{align*}
K_\vtheta(\vx) & = \prod_{j=1}^d [1 + a' B_{2r'}(x_j)] \\
& = \prod_{j=1}^d [1 + a \kappa_r(x_j)], \quad  \kappa_r(x) = \sum_{\abs{k} \ge 1}\frac{\exp(2\pi\sqrt{-1} kx)}{\abs{k}^{r}}, \quad
\vtheta = (a,r) \in (0,\infty) \times (1,\infty), \quad r = 2r'
\end{align*}
First expression is simple closed form in terms of Bernoulli polynomials, \alert{but $r' \in \naturals$} \\
Second expression has flexible $r>1$, \alert{but involves infinite sum}

\uncover<2->{An alternative, for which the FBT and the evaluation of $\vlambda$ are \alert{the same asymptotic cost}
\[
K_{\vtheta,n}(\vx) 
= \prod_{j=1}^d \biggl [1 + a \kappa_{r,n}(x_j)   \biggr], \quad \kappa_{r,n}(x) = \sum_{\alert{1  \le  \abs{k} \le n/2}} \frac{\exp(2\pi\sqrt{-1} kx)}{\abs{k}^{r}},\quad \vtheta = (a,r) \in (0,\infty) \times (1,\infty)
\]
but now the \alert{kernel changes with $n$}, and $K_{\vtheta,n} \to K_{\vtheta}$ as $ n\to \infty$}

\end{frame}



\begin{frame}{Numerical Examples}
 
 
 
 
 
\end{frame}


\section{Gaussian Process Diagnostics}

\begin{frame}{Is $f$ a Typical Instance of a Gaussian Process?}
	content...
\end{frame}

\section{Summary}

\vspace{-6ex}

\begin{frame}
	{What Are the Right Ingredients for Adaptive Function Approximation?}
	
	\begin{itemize}
		\item A fixed budget \alert{homogeneous approximation}, $\app : \cx^n \times \reals^n \to L^{\infty}(\cx)$, with an error bound, e.g., linear splines, RKHS approximation
		
		\item An unbounded, non-convex \alert{candidate set}, $\cc$, for which the error bound can be bounded in \alert{data-driven} way; what you see is almost what you get
		
		\item \alert{Necessary} conditions for $f$ to lie in $\cc$; will not have sufficient conditions
		
		\item A rich enough candidate set from which the right approximation can be \alert{inferred}; attention to underfitting and overfitting
		
			
	\end{itemize}


	\uncover<2->{
	\begin{itemize}
	\item More work is needed on 
	
	\begin{itemize}
		
		\item What makes a good initial sample
	
		\item Balancing the richness of the candidate set with overfitting
		
		\item Numerical instability and computational effort challenges for larger numbers of data sites. 
	
	
\end{itemize}
\end{itemize}
}


\end{frame}



\finalthanksnote{These slides are  available at \\  \href{https://speakerdeck.com/fjhickernell/right-ingredients-for-adaptive-function-approximation}{\nolinkurl{speakerdeck.com/fjhickernell/???}}}


\thankyouframe


\begin{frame}
	\frametitle{References}
\printbibliography
\end{frame}





\begin{frame}{Computing $\kappa(x)$ for $x = 0, 1/n \ldots, 1-1/n$}
	\begin{align*}
	\zeta(s,a) &: = \sum_{m=0}^\infty (a+m)^{-s}  \quad \talert{Hurwitz zeta function}\\
	\kappa(i/n) & = \sum_{\abs{k} \ge 1} \frac{\exp(2\pi\sqrt{-1} ki/n)}{\abs{k}^{r}} \\
	& = \frac{2}{n^{r}}  \sum_{m=1}^\infty \frac{1}{\abs{m}^{r}} 
	+ 
	\sum_{k=-n/2}^{- 1}  \sum_{m=-\infty}^{-\infty} \frac{\exp(2\pi\sqrt{-1} ki/n)}{\abs{k+mn}^{r}} 
	+ \sum_{k=1}^{n/2-`}   \sum_{m=-\infty}^{-\infty} \frac{\exp(2\pi\sqrt{-1} ki/n)}{\abs{k+mn}^{r}}  \\
	& = \frac{1}{n^{r}} \Biggl\{2\zeta(r)
	+ 
	\sum_{k=-n/2}^{- 1} \exp(2\pi\sqrt{-1} ki/n) \bigl[ \zeta(r,\abs{k}/n) + \zeta(r,1-\abs{k}/n)\bigr] \\
	&\qquad \qquad + 
	\sum_{k=1}^{n/2- 1} \exp(2\pi\sqrt{-1} ki/n) \bigl[ \zeta(r,\abs{k}/n) + \zeta(r,1-\abs{k}/n)\bigr] 
	\end{align*}
\end{frame}

\end{document}


\begin{frame}{What Should $\fC(\mX)$ Be?}
	\[
	\cc  =  \bigl\{ f \in \cf :  \norm[\cf]{f - \app(\mX,\vy)} \le  \alert{\fC(\mX)} \norm[\cf]{f} \bigr\}
	\]
	As $\mX$ covers $\cx$ better, $\fC(\mX)$ should decrease.  Suggest choosing
	\begin{align*}
	\frac{ \norm[\cf]{f - \app(\mX,\vy)}^2 }{ \norm[\cf]{f}^2 } = \fC^2(\mX) & : = \fC_0^2 \norm[\infty]{\frac{\sup \{ \abs{f(\cdot)}^2 : \norm[\cf]{f} \le 1,   \ f(\mX) = \vzero  \}}{\sup \{ \abs{f(\cdot)}^2 : \norm[\cf]{f} \le 1 \}}} \\
	&  = \fC_0^2  \norm[\infty]{\frac{K(\cdot,\cdot) -  K(\cdot,\mX)\bigl(K(\mX,\mX) \bigr)^{-1} K(\mX, \cdot)}{K(\cdot,\cdot)}}
	\end{align*}
	
	
\end{frame}


